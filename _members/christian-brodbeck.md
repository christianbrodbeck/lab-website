---
name: Christian Brodbeck
image: https://www.eng.mcmaster.ca//app/uploads/2023/12/Christian_Brodbeck_Headshot-400x400.jpg
role: pi
affiliation: McMaster University
aliases:
  - Christian Brodbeck
  - C. Brodbeck
  - C Brodbeck
links:
  orcid: 0000-0001-8380-639X
  github: christianbrodbeck
  google-scholar: m2x515IAAAAJ
  cv: https://drive.google.com/file/d/1RQG5w4C2Nf9kQCQJMB5qNDg_0bKOqreD/view?usp=share_link
  email: brodbecc@mcmaster.ca
---

# Affiliations

 - Home department: [Computing and Software](http://cas.mcmaster.ca)
 - Member: Centre for Advanced Research in Experimental and Applied Linguistics ([ARiEAL](https://arieal.humanities.mcmaster.ca))
 - Associate member: [Neuroscience Graduate Program](https://neuroscience.mcmaster.ca)
 - Associate member: McMaster Institute for Research on Aging ([MIRA](https://mira.mcmaster.ca))

# Research interests

When humans listen to speech, 
the acoustic signal that enters the ears is a complex pattern of air pressure fluctuations.
Yet, listeners intuitively and almost instantaneously experience meaning in these sounds. 
My research focuses on the transformations that happen in the brain to enable this.

The goal of my research is to understand and measure how the brain processes speech.
I am particularly interested in how people comprehend speech in realistic settings, 
including continuous, meaningful speech, and speech in noisy backgrounds.
For this I primarily work with electrophysiological brain signals
([MEG](https://en.wikipedia.org/wiki/Magnetoencephalography) 
& [EEG](https://en.wikipedia.org/wiki/Electroencephalography))
and computational models. 
M/EEG allow us to measure brain activity with millisecond resolution, required 
for capturing brain responses to rapidly evolving speech signals.
Computational models of speech recognition allow us to better understand 
the transformations necessary for recognizing speech, 
and they also allow us to make quantitative predictions for brain activity.

This work lays the foundations for better understanding how speech perception
is affected in different settings and populations.
For example, how does speech processing change with age?
How is it affected by a hearing impairment?
And how can this guide us to better address challenges faced by these populations.

I use Python to develop tools to make this research possible, 
and many of those tools are available in the open source libraries
[MNE-Python](https://mne.tools) and [Eelbrain](https://eelbrain.readthedocs.io).
For an introduction to analyzing M/EEG responses in experiments 
with continuous designs, such as audiobook listening or movie watching,
see our recent [eLife paper](https://doi.org/10.7554/eLife.85012). 
